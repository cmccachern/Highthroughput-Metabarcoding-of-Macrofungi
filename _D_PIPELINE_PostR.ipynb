{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkslategray>Illumina Data Processing (Post-R)\n",
    "### <font color=darkslategray>*also known as \"Pipeline 4\"*\n",
    "#### <font color=darkgreen>Jessie Berta-Thompson & Gary Olds\n",
    "#### <font color=green>Designed for DBG fungal DNA barcoding projects\n",
    "##### <font color=mediumseagreen>Last Edited: 2021/05/07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=magenta>CHECKPOINT\n",
    "##### <font color=rebeccapurple>*A local working directory (folder) must be set up with the following instructions exactly in order for code in this notebook to run correctly.*\n",
    "\n",
    "### <font color=dodgerblue>*Program Prep*\n",
    "Make sure the appropriate programs & tools are installed & imported.\n",
    "\n",
    "**See ComputerTracking.doc for details.**\n",
    "\n",
    "<font color=red>This version works specifically for Gary's computer and requires extra downloads to run linux and mac-related things on a PC. See separate doc for computer setup..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=dodgerblue>*Complete Directory Naming*\n",
    "Name all directories needed throughout the workflow (only edit MAINdir, all others are automatic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPdir = \"C:\\\\Users\\\\cgogo\\\\Documents\\\\PIPELINE\\\\MICHIGAN\"\n",
    "MAINdir = \"C:\\\\Users\\\\cgogo\\\\Documents\\\\PIPELINE\\\\MICHIGAN\\\\SplitSamples\"\n",
    "TRIMMEDdir = f\"{MAINdir}\\\\NoPrimers\"\n",
    "PROCESSEDdir = f\"{TRIMMEDdir}\\\\Filter_Trim\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse dada2 inference table for most abundant fasta and fact table\n",
    "\n",
    "**Goals:**<br>\n",
    "Simple fasta output of most abundant sequence per specimen from dada2 inference table.<br>\n",
    "Fact table, one line per specimen, about process and about sequences. <br>\n",
    "**Dataset:**<br>\n",
    "Testing on Owl pooled data, run with maxEE 2,2 per-sample bimera handling dada2 pipeline.<br>\n",
    "But writing to be a re-usable, shareable tool for Gary to use on lots of datasets while running methods comparisons.<br>\n",
    "\n",
    "**Working off code from here for a start:**<br>\n",
    "/Users/jessie/Dropbox/jessie/owl_run/post_processing_dada2_sequence_tables/Examine_sequence_tables.ipynb<br>\n",
    "Handles inference table parsing and fasta write-out, but not summary info.<br>\n",
    "<br>\n",
    "<br>\n",
    "4-28-21<br>\n",
    "4-29-21<br>\n",
    "Jessie B-T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=magenta>CHECKPOINT\n",
    "##### <font color=rebeccapurple>*Import tools & load Python packages*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##    BASICS\n",
    "\n",
    "import os                #command-line like functions, for operating system interface (finding files)\n",
    "import subprocess        #recommended way of running command line programs from within python\n",
    "import numpy as np       #for math and arrays\n",
    "import shutil            #used to move files around\n",
    "import sys               #helps with reading and writing onto text files\n",
    "from tqdm import tqdm    #a progress bar for for-loops (lets you see progress in actively running loops)\n",
    "from time import time    #use to track time and measure how long chunks are taking to run\n",
    "import shutil            #for moving files from directory to directory\n",
    "import csv               #writing and reading csv\n",
    "\n",
    "##    FOR DNA\n",
    "\n",
    "from Bio import Seq                 #reading in and manipulating sequence data\n",
    "from Bio.Seq import Seq             #function that creates a sequence object (string with associated alphabet)\n",
    "                                    #Bio.Seq is a sub-part of Bio, Seq is one of the functions within Bio.Seq\n",
    "from Bio import SeqIO               #reading in and saving out sequence files (such as fasta)\n",
    "from Bio.SeqRecord import SeqRecord #create sequence with associated alphabet AND associated label/ID and other info\n",
    "from Bio.SeqUtils import GC         #for calculating GC content\n",
    "\n",
    "##    FOR FIGURES\n",
    "\n",
    "import matplotlib.pyplot as plt          #basic plotting tools that let you do most of what you'll need to do, set up as a shortcut\n",
    "import matplotlib                        #get ALL of matplotlib for fancier tools\n",
    "                                         #add specific other matplotlib imports as needed\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42 #make saved vector pdf files use fonts instead of lines/shapes for text\n",
    "                                         #(essential weird line for saving vector pdfs for downstream editing in illustrator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=magenta>CHECKPOINT\n",
    "##### <font color=rebeccapurple>*Define several new functions for the pipeline*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to read in file                 ### the inferences tables are saved as CSV files, tab-delimited\n",
    "def csv2dict(infilename):\n",
    "    \"\"\"\n",
    "    Description\n",
    "        Read in a csv with sequencing info and build a dictionary,\n",
    "        each key a column header, each value a list of strings with column\n",
    "        contents (in original row order). Input file has one row for each\n",
    "        specimen, with species name, identifying numbers, sequences and notes\n",
    "        in columns.\n",
    "    Input: File name\n",
    "    Output: Data dictionary\n",
    "    Example: plate1 = csv2dict(SMHF_DNA_1.csv)\n",
    "    \"\"\"\n",
    "    #Initialize empty data dictionary\n",
    "    datadict = {}\n",
    "    #Open file\n",
    "    print(\"Opening file {}\".format(infilename))\n",
    "    with open(infilename) as csv_file:\n",
    "        \"\"\"\n",
    "        Uses csv DictReader tool to parse file.\n",
    "        This method good at automatically dealing with excel quirks\n",
    "        (line endings weird, incomplete rows not always filled out)\n",
    "        \"\"\"\n",
    "        csv_reader = csv.DictReader(csv_file, delimiter='\\t') #tabs!   since it is a tab-delimited file\n",
    "        headers = csv_reader.fieldnames #What are the columns?\n",
    "        \"\"\"\n",
    "        Default DictReader dictionary format annoying; Each row a different\n",
    "        dictionary with the same headers. Make a new single dictionary, each\n",
    "        column a list in a dictionary.\n",
    "        \"\"\"\n",
    "        # Loop over headers from csv reader to make them keys of new dictionary\n",
    "        for header in headers:\n",
    "            datadict[header] = [] #make values empty rows for now\n",
    "        # Loop over rows (dictionaries) in csv reader object, representing lines\n",
    "        for row in csv_reader:\n",
    "            # Loop over columns\n",
    "            for header in headers:\n",
    "                # Fill in lists with entry for this row,column from csv reader\n",
    "                entry = row[header]\n",
    "                #Remove trailing/leading whitespace (occurs in database)\n",
    "                if entry is not None:\n",
    "                    datadict[header].append(entry.strip())\n",
    "                # Can't strip whitespace from NoneType objects of empty fields\n",
    "                else:\n",
    "                    datadict[header].append(entry)\n",
    "    print(f\"Created data dictionary. {len(datadict[headers[0]])} rows, {len(headers)} columns\")\n",
    "    return(datadict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a function to write a data dictionary out to tab-delimited text file\n",
    "def savedict(indict, outfile):\n",
    "    \"\"\"\n",
    "    Save a dictionary of form header_key:data_list\n",
    "    to tab-delimited file, given dictionary and file name string.\n",
    "    Creates a table with columns representing keys and rows items in value lists.\n",
    "    \"\"\"\n",
    "    #Get keys into a stable list\n",
    "    ordered_keys = list(indict.keys())\n",
    "\n",
    "    #Initialize list of lines to save with header line based on keys\n",
    "    filerows = [\"\\t\".join(ordered_keys)]\n",
    "\n",
    "    #Example key (use first one)\n",
    "    examplekey = ordered_keys[0]\n",
    "\n",
    "    #Loop over rows in dataset\n",
    "    for i in range(len(indict[examplekey])):\n",
    "        #place to store entries for each column for this row as a list\n",
    "        thisrow = []\n",
    "        #Loop over columns\n",
    "        for key in ordered_keys:\n",
    "            thisrow.append(str(indict[key][i]))#for join method, has to be a string.\n",
    "\n",
    "        #Join items in a file line\n",
    "        filerows.append(\"\\t\".join(thisrow))\n",
    "\n",
    "    #Save lines to file\n",
    "    with open(outfile, 'w') as out:\n",
    "        for line in filerows:\n",
    "            out.write(line+\"\\n\")\n",
    "    # Report back\n",
    "    print(f\"Saving dictionary to file {outfile}, {(len(filerows)-1)} lines of data, {len(indict.keys())} columns.\")\n",
    "    # Return nothing\n",
    "    return(None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=magenta>CHECKPOINT\n",
    "##### <font color=rebeccapurple>*Read in and format inference table data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read set dada2 inference table results\n",
    "\n",
    "MICH = csv2dict(f\"{UPdir}\\\\7d_inference_table_chimerasremoved.txt\")\n",
    "# also prints it out, tells how many rows and columns in the file\n",
    "\n",
    "### Every unique sequence in the WHOLE dataset gets its own column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at first few keys\n",
    "print(list(MICH.keys())[0:5])    # print the first keys, so the first 5 column headers\n",
    "print(\"\")\n",
    "print(MICH[\"Filename\"][0])  #print the first item in the filename column\n",
    "print(\"\")\n",
    "print(list(MICH.keys())[2])                    ## for all keys in this dictionary, what is key 2 (which means the third column header here...)\n",
    "print(MICH[list(MICH.keys())[2]][0:10])       ## in key 2 (column 3), what are the first 10 items (really its 0-9 which means rows 1 through 10 (after header row))\n",
    "print(MICH[list(MICH.keys())[0]][0:10])       ## in key 0 (column 1), what are the first 10 items (\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull column names for sequences (the sequences!)\n",
    "def getseqkeys(inference):\n",
    "    sequencekeys = []\n",
    "    for key in inference.keys():\n",
    "        if key not in ['Specimen', 'Filename', 'UniqueSeqCount', 'TotalCount']:\n",
    "            sequencekeys.append(key)\n",
    "    return(sequencekeys)\n",
    "\n",
    "MICH_seqs = getseqkeys(MICH)\n",
    "print(len(MICH_seqs))\n",
    "\n",
    "## how many unique sequences are there, so in all of the keys, how many keys are NOT the four listed above.\n",
    "    # UniqueSeqCount and TotalCount do not exist quite yet, but they may in the future, so clarify.\n",
    "## there are 485 columns. 483 of those columns are literal sequences, and then two are names like \"Specimen\" and \"Filename\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make numbers numbers in table (csv2dict leaves input as strings)     ## make sure \"0\" is actually literally the number zero\n",
    "def str2int(inference):\n",
    "    for key in getseqkeys(inference):\n",
    "        newcol = []\n",
    "        for item in inference[key]:\n",
    "            newcol.append(int(item))\n",
    "        inference[key] = newcol\n",
    "    return(inference)\n",
    "\n",
    "MICH = str2int(MICH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Summary Columns:\n",
    "#     How many unique sequences per specimen?\n",
    "#     How many total abundance counts per specimen?\n",
    "\n",
    "def addSums(inference):                  # define a new function called addSums\n",
    "    inference['UniqueSeqCount'] = []     # making a new key (column) called UniqueSeqCount, which right now is an empty list\n",
    "    inference['TotalCount'] = []         # making a new key (column) called TotalCount, which right now is an empty list\n",
    "    for i in range(len(inference['Specimen'])):  # loop over the total number of rows (using the first column arbitrarily)\n",
    "        this_row_nonzero_counts = []             # going to store non-zero abundance counts in this new list\n",
    "        for seq in getseqkeys(inference):        # looping over the sequence columns\n",
    "            if inference[seq][i]!=0:             # for each given sequence and each given specimen (one cell (columnxrow) at a time),  if the value is NOT zero (\"!=0\" = evaluate to true if not zero, false if zero)\n",
    "                this_row_nonzero_counts.append(inference[seq][i])  # then, for those in which the value is TRUE, then add whatever that abundance count was to this list\n",
    "        inference['UniqueSeqCount'].append(len(this_row_nonzero_counts)) # for this same specimen, how many items (abundance values) were found and added to that list (**how many unique sequences per specimens**)\n",
    "        inference['TotalCount'].append(sum(this_row_nonzero_counts))     # for this specimen, how many sequences were there total (\"how many survived pipeline\")       (**how many sequences per specimen**)\n",
    "    return(inference)                    # function starts with being defined, and ends here with the output being... outputted (\"modified dictionary\" cuz now there are two more columns)\n",
    "\n",
    "MICH = addSums(MICH)   #MICH is being replaced by the new and modified MICH\n",
    "\n",
    "savedict(MICH,\"9_InferenceTable_SummaryCounts.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the number of unique sequences, number of specimens, and number of specimens with no sequences for each dataset\n",
    "print(\"Unique seqs\\tSpecimens\\tSpecimens w/ 0 seqs\\tSpecimens w/ <10 reads\\tSpecimens w/ <150 reads\")\n",
    "print(f\"{len(getseqkeys(MICH))}\\t{len(MICH['Specimen'])}\\t{MICH['UniqueSeqCount'].count(0)}\\t{len([i for i in MICH['TotalCount'] if i<10])}\\t{len([i for i in MICH['TotalCount'] if i<150])}\")\n",
    "\n",
    "## \"Specimens  w/ 0 seqs\" are specimens that lost ALL sequences in processing, and didn't survive the pipeline\n",
    "## \"Specimens w <10 seqs\" in order to look at specimens with very low counts and not a lot support it\n",
    "## low read counts - less trust worthy; threshold or number is arbitrary until/unless further digging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=magenta>CHECKPOINT\n",
    "##### <font color=rebeccapurple>*Make figures of number of unique sequences per specimen!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=magenta>CHECKPOINT\n",
    "##### <font color=rebeccapurple>*Save out most abundant sequence & summary info for each specimen*\n",
    "For now, getting 1 representative sequence per specimen. Take first instance (arbitrary) in cases of abundance ties. Other sequences are  relevant too, e.g. biological variants, explanations for weird stuff, but for a single sequence rep this seems like a reasonable approach. Some ways to handle this depth of other data will be to get answers for this top sequence, then follow up for problematic sequences, and do dedicated analyses for more sequences across whole dataset, but we have to start somewhere!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveOneMostAbundantTiesFirst(inference, label):\n",
    "    print(label)\n",
    "    \n",
    "    #Set up a dictionary to hold summary info about process & sequences\n",
    "    summary = {}\n",
    "    summary['Specimen'] = []                                     ## specimen name\n",
    "    summary['Dataset version'] = []                              ## specify which dataset (aka OWL_22, are you doing QR, HM, what?? specify here!)\n",
    "    summary['Any dada2 results?'] = []                           ## what specimens had any results at all, like who survived\n",
    "    summary['Total dada2 read abundance for specimen'] = []      ## how many total reads/sequences per specimen\n",
    "    summary['Total dada2 unique sequences for specimen'] = []    ## how many unique sequences per specimen (ex: 12 reads worth of one unique sequence)\n",
    "    summary['Most abundant sequence'] = []                       ## which of those unique sequences had the highest abundance (most reads)\n",
    "    summary['Fasta id'] = []                                     ## save the newly formed fasta file's ID to this table\n",
    "    summary['Fasta file'] = []                                   ## the name of the new fasta file\n",
    "    summary['Abundance of most abundant'] = []                   ## how many reads were there of that one, top unique seq\n",
    "    summary['Tie for most abundant?'] = []                       ## were there multiple most-abundant? Abritrairly take the first one\n",
    "    summary['Sequence length'] = []                              ## how long is this chosen sequence\n",
    "    summary['Sequence GC content'] = []                          ## what is the GC content of the chosen sequence\n",
    "\n",
    "    single_sequence_list = [] #store 1 most abundant sequence per specimen (that has any sequences) here\n",
    "\n",
    "    #Loop over specimens for dataset\n",
    "    for i in range(len(inference['Specimen'])):       #looping over the length of the table, so just loop over each row\n",
    "        summary['Specimen'].append(inference['Specimen'][i]) #store specimen name (populate Specimen key)\n",
    "        summary['Dataset version'].append(label)              \n",
    "        summary['Total dada2 read abundance for specimen'].append(inference['TotalCount'][i])\n",
    "        summary['Total dada2 unique sequences for specimen'].append(inference['UniqueSeqCount'][i])\n",
    "        ## ABOVE: just copy over the values we've already found earlier in the code\n",
    "        \n",
    "        #Create matched lists of the sequences for this specimen and their abundances\n",
    "        this_one_seqs = []         #for this specimen, what unique sequences did it come back with\n",
    "        this_one_seq_counts = []   #for this specimen, what were the read counts per the above unique sequences\n",
    "        for seq in getseqkeys(inference):\n",
    "            thisseqthisspeccount = inference[seq][i]\n",
    "            if thisseqthisspeccount != 0:    #if TRUE that it isn't zero\n",
    "                this_one_seqs.append(seq)\n",
    "                this_one_seq_counts.append(thisseqthisspeccount)\n",
    "\n",
    "        #Check that there are sequences for this specimen\n",
    "        if len(this_one_seqs) == 0:           # does the specimen have any sequences at all? (did it survive processing)\n",
    "            #No sequences? fill up summary table with boring stuff\n",
    "               ## replace zeros with NA to ignore that specimen cuz the zeros are not meaningful here\n",
    "            summary['Any dada2 results?'].append('No sequences left after dada2 pipeline.')\n",
    "            summary['Most abundant sequence'].append('NA')\n",
    "            summary['Fasta id'].append('NA')\n",
    "            summary['Fasta file'].append('NA')\n",
    "            summary['Abundance of most abundant'].append('NA')\n",
    "            summary['Tie for most abundant?'].append('NA')\n",
    "            summary['Sequence length'].append('NA')\n",
    "            summary['Sequence GC content'].append('NA')\n",
    "        \n",
    "        else:      #if the above IF is NOT true, then is THIS true? So, if the rwo DOES have something (and didn't get filled with NA)...\n",
    "            #Find the most abundant sequence's abundance\n",
    "            max_value = max(this_one_seq_counts)    # ...then find the most abundant sequence by finding the max of all the reads per unique sequence\n",
    "\n",
    "            #Check for a tie, if exactly one instance of max:\n",
    "            if this_one_seq_counts.count(max_value)==1:  # if the instance of the maximum count value is 1, then there's one. Great, not a tie.\n",
    "                tienote = \"Not a tie, single most abundant sequence found.\"\n",
    "                #Get index of max value\n",
    "                max_index = this_one_seq_counts.index(max_value)   # use matching index to find what the sequence is based on where in the list it is\n",
    "                max_seq = this_one_seqs[max_index]                 # using the matching index, get the actual sequence itself\n",
    "                #Create a fasta sequence name\n",
    "                name = inference['Specimen'][i]+\"_\"+label+\"_most_abundant_seq_\"+str(max_value)+\"x\"  # generate a FASTA header\n",
    "                #Create a sequence record\n",
    "                record = SeqRecord(Seq(max_seq),id=name,name=\"\",description=\"\")   # turn into a sequence record (an object, not just a string) so that we can manipulate it in code later on. (description comes after the space (after the header))\n",
    "                #Store sequence into the sequence list\n",
    "                single_sequence_list.append(record)\n",
    "\n",
    "            else:\n",
    "                print(f\"{this_one_seq_counts.count(max_value)} sequences for specimen {inference['Specimen'][i]} tied for max abundance at {max_value}x, saving first one.\")\n",
    "                #ABOVE: if there IS a tie somewhere, print it out. If there are no ties, nothing will be printed out here. (so if there multiple (>1) occurences of the max value)\n",
    "                #Get index of max value: this gets first instance\n",
    "                max_index = this_one_seq_counts.index(max_value)\n",
    "                max_seq = this_one_seqs[max_index]\n",
    "                #Create a fasta sequence name\n",
    "                name = inference['Specimen'][i]+\"_\"+label+\"_tied_most_abundant_seq_\"+str(max_value)+\"x\"\n",
    "                #Create a sequence record\n",
    "                record = SeqRecord(Seq(max_seq),id=name,name=\"\",description=\"\")\n",
    "                #Store sequence in a list\n",
    "                single_sequence_list.append(record)\n",
    "                \n",
    "                \n",
    "                #all sequences for max value\n",
    "                all_indices = np.where(np.array(this_one_seq_counts) == max_value)[0]\n",
    "                tied_seqs = []\n",
    "                for z in all_indices:\n",
    "                    tied_seqs.append(this_one_seqs[z])\n",
    "                \n",
    "                tienote = f\"{this_one_seq_counts.count(max_value)} sequences tied for max abundance at {max_value}x, used first: {','.join(tied_seqs)}\"\n",
    "\n",
    "            #Populate summary table (one line per specimen) for the specimns that DID have sequences (did survive, have unique seqs)\n",
    "            summary['Any dada2 results?'].append('Yes')                    # \"it did survive processing\"\n",
    "            summary['Most abundant sequence'].append(max_seq)              # what is the sequence\n",
    "            summary['Fasta id'].append(name)                               # the fasta header (variable called \"name\")\n",
    "            summary['Abundance of most abundant'].append(max_value)        # How many reads were there of this sequence\n",
    "            summary['Sequence length'].append(len(max_seq))                # length (# basepairs) of sequence\n",
    "            summary['Sequence GC content'].append(f\"{GC(max_seq):.1f}\")    # % GC content of sequence, round to 1 decimal place\n",
    "            summary['Tie for most abundant?'].append(tienote)              # provide the sentence made above if there is a tie\n",
    "\n",
    "    #Write out sequences to fasta file\n",
    "    outpath = label+f\"_single_most_abundant_sequence_for_{len(single_sequence_list)}_specimens_first_instance_in_ties.fasta\"   # name includes how many seqs are in it\n",
    "    SeqIO.write(single_sequence_list,outpath, \"fasta\")   # writes out the sequences into the fasta file\n",
    "    print(f\"Write {len(single_sequence_list)} sequences to {outpath}\")\n",
    "    \n",
    "    for j in range(len(inference['Specimen'])):\n",
    "        summary['Fasta file'].append(outpath)              \n",
    "    \n",
    "    #Write out summary dictionary to tab-delimited text file\n",
    "    summary_outfile = label+f\"_single_most_abundant_sequence_for_{len(single_sequence_list)}_specimens_first_instance_in_ties_summary.txt\"\n",
    "    savedict(summary, summary_outfile)\n",
    "    \n",
    "    #In case you want to explore data right away, send out summary table and sequences\n",
    "    return(summary, single_sequence_list)           #return summary dictionary and lists to wok with them later\n",
    "    \n",
    "#Run function! \n",
    "MICH_summary, MICH_most_abundant_seqs = saveOneMostAbundantTiesFirst(MICH, \"MICH\")\n",
    "\n",
    "## think about how to manage everything in excel workbooks etc since there are a lot of new spreadsheets to work with here\n",
    "\n",
    "## find % : abundant of most abundant / total abundance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
